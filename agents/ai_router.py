"""
AI Router - —É–º–Ω—ã–π –º–∞—Ä—à—Ä—É—Ç–∏–∑–∞—Ç–æ—Ä –∑–∞–ø—Ä–æ—Å–æ–≤ –∫ —Ä–∞–∑–Ω—ã–º AI –º–æ–¥–µ–ª—è–º
–ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –≤—ã–±–∏—Ä–∞–µ—Ç –æ–ø—Ç–∏–º–∞–ª—å–Ω—É—é –º–æ–¥–µ–ª—å –ø–æ —Ç–∏–ø—É –∑–∞–¥–∞—á–∏, —Å–ª–æ–∂–Ω–æ—Å—Ç–∏ –∏ –±—é–¥–∂–µ—Ç—É
"""
import os
from typing import Literal, Optional, Dict, Any, List
import logging
from dotenv import load_dotenv
from threading import Lock
from collections import defaultdict, deque
from time import time
import anthropic
from openai import OpenAI
import google.generativeai as genai
try:
    from .database import get_db
except Exception:
    from database import get_db

# –ó–∞–≥—Ä—É–∑–∫–∞ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã—Ö –æ–∫—Ä—É–∂–µ–Ω–∏—è
load_dotenv()

logger = logging.getLogger(__name__)

# –ú–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ –¥–æ—Å—Ç—É–ø–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π –¥–ª—è –æ—Ü–µ–Ω–∏–≤–∞–Ω–∏—è
MODELS: List[Dict[str, Any]] = [
    {
        'name': 'claude-sonnet-4-20250514',
        'best_for': ['architecture', 'research', 'code', 'chat'],
        'tier': 'premium'
    },
    {
        'name': 'gpt-4-turbo',
        'best_for': ['code', 'test', 'chat', 'general'],
        'tier': 'premium'
    },
    {
        'name': 'gpt-4o',
        'best_for': ['chat', 'vision', 'general'],
        'tier': 'premium'
    },
    {
        'name': 'deepseek/deepseek-chat',
        'best_for': ['code', 'devops', 'review'],
        'tier': 'advanced'
    },
    {
        'name': 'gemini-2.0-flash',
        'best_for': ['review', 'test', 'chat'],
        'tier': 'standard'
    },
    {
        'name': 'ollama',
        'best_for': ['chat', 'code'],
        'tier': 'basic'
    },
]

class AIRouter:
    """–¶–µ–Ω—Ç—Ä–∞–ª—å–Ω—ã–π —Ä–æ—É—Ç–µ—Ä –¥–ª—è –≤—Å–µ—Ö AI –º–æ–¥–µ–ª–µ–π"""
    
    def __init__(self):
        """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –∫–ª–∏–µ–Ω—Ç–æ–≤ AI –º–æ–¥–µ–ª–µ–π"""
        # Anthropic (Claude)
        anthropic_key = os.getenv("ANTHROPIC_API_KEY")
        self.claude = anthropic.Anthropic(api_key=anthropic_key) if anthropic_key else None
        
        # OpenAI (GPT)
        openai_key = os.getenv("OPENAI_API_KEY")
        self.openai = OpenAI(api_key=openai_key) if openai_key else None
        
        # OpenRouter (–¥–æ—Å—Ç—É–ø –∫–æ –º–Ω–æ–≥–∏–º –º–æ–¥–µ–ª—è–º)
        openrouter_key = os.getenv("OPENROUTER_API_KEY")
        self.openrouter = OpenAI(
            base_url="https://openrouter.ai/api/v1",
            api_key=openrouter_key
        ) if openrouter_key else None
        
        # Google Gemini
        gemini_key = os.getenv("GOOGLE_AI_API_KEY")
        if gemini_key:
            genai.configure(api_key=gemini_key)
            self.gemini = genai.GenerativeModel('gemini-2.0-flash-exp')
        else:
            self.gemini = None
        
        # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è
        self.stats = {
            'calls': 0,
            'tokens': 0,
            'cost': 0.0,
            'by_model': {}
        }
        
        # –ú–æ–¥–µ–ª–∏ –∏ –ë–î
        self.models = MODELS
        self.db = get_db()

        # Rate limiting
        self._rate_limits = defaultdict(lambda: deque())
        self._lock = Lock()
        # –õ–∏–º–∏—Ç—ã –ø–æ –º–æ–¥–µ–ª—è–º (–∑–∞–ø—Ä–æ—Å–æ–≤ –≤ –º–∏–Ω—É—Ç—É)
        self._model_limits = {
            'gemini-2.0-flash': 60,
            'grok-beta': 30,
            'deepseek/deepseek-chat': 30,
            'anthropic/claude-3.5-sonnet': 50,
            'openai/gpt-4o': 50,
            'openai/gpt-4o-mini': 60,
            'ollama': 100
        }
        logger.info(f"AI Router initialized with {len(self.models)} models")

    def _check_rate_limit(self, model_name: str, wait: bool = True) -> bool:
        """
        –ü—Ä–æ–≤–µ—Ä–∏—Ç—å rate limit –¥–ª—è –º–æ–¥–µ–ª–∏
        """
        with self._lock:
            current_time = time()
            limit = self._model_limits.get(model_name, 30)

            # –û—á–∏—Å—Ç–∫–∞ —Å—Ç–∞—Ä—ã—Ö –æ—Ç–º–µ—Ç–æ–∫ (—Å—Ç–∞—Ä—à–µ 60—Å)
            queue = self._rate_limits[model_name]
            while queue and current_time - queue[0] > 60:
                queue.popleft()

            # –ü—Ä–µ–≤—ã—à–µ–Ω–∏–µ –ª–∏–º–∏—Ç–∞
            if len(queue) >= limit:
                if wait:
                    oldest = queue[0]
                    wait_time = 60 - (current_time - oldest)
                    if wait_time > 0:
                        logger.warning(f"‚è±Ô∏è Rate limit hit for {model_name}, waiting {wait_time:.1f}s...")
                        import time as time_module
                        time_module.sleep(wait_time)
                        return self._check_rate_limit(model_name, wait=False)
                else:
                    logger.warning(f"üö´ Rate limit exceeded for {model_name}")
                    return False

            # –†–µ–≥–∏—Å—Ç—Ä–∏—Ä—É–µ–º —Ç–µ–∫—É—â–∏–π –≤—ã–∑–æ–≤
            queue.append(current_time)
            return True

    def get_rate_limit_stats(self) -> Dict:
        """–ü–æ–ª—É—á–∏—Ç—å —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É rate limiting"""
        with self._lock:
            stats: Dict[str, Dict[str, float]] = {}
            current_time = time()
            for model_name, queue in self._rate_limits.items():
                while queue and current_time - queue[0] > 60:
                    queue.popleft()
                limit = self._model_limits.get(model_name, 30)
                used = len(queue)
                available = max(0, limit - used)
                stats[model_name] = {
                    'limit': limit,
                    'used': used,
                    'available': available,
                    'usage_percent': round((used / limit) * 100, 1) if limit else 0.0,
                }
            return stats

    def _get_model_performance(self, model: str, task_type: str = None) -> Dict:
        """
        –ü–æ–ª—É—á–∏—Ç—å —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –º–æ–¥–µ–ª–∏
        Returns dict: success_rate, avg_cost, avg_tokens, total_uses
        """
        try:
            import sqlite3
            
            query = """
                SELECT 
                    COUNT(*) as total_uses,
                    AVG(CASE WHEN error = 0 THEN 1 ELSE 0 END) as success_rate,
                    AVG(cost) as avg_cost,
                    AVG(tokens) as avg_tokens
                FROM requests
                WHERE model = ?
            """
            params: List[Any] = [model]
            
            if task_type:
                query += " AND task_type = ?"
                params.append(task_type)
            
            with sqlite3.connect(self.db.db_path) as conn:
                conn.row_factory = sqlite3.Row
                cursor = conn.execute(query, params)
                result = dict(cursor.fetchone())
                
                if not result or result.get('total_uses', 0) == 0:
                    return {
                        'total_uses': 0,
                        'success_rate': 1.0,
                        'avg_cost': 0.0,
                        'avg_tokens': 0
                    }
                return result
        except Exception as e:
            logger.error(f"Error getting model performance: {e}")
            return {
                'total_uses': 0,
                'success_rate': 1.0,
                'avg_cost': 0.0,
                'avg_tokens': 0
            }

    def _calculate_model_score(
        self,
        model: Dict,
        task_type: str,
        complexity: int,
        budget: str
    ) -> float:
        """
        –†–∞—Å—Å—á–∏—Ç–∞—Ç—å score –º–æ–¥–µ–ª–∏ –¥–ª—è –¥–∞–Ω–Ω–æ–π –∑–∞–¥–∞—á–∏
        """
        score = 0.0
        
        # 1) –°–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏–µ —Ç–∏–ø—É –∑–∞–¥–∞—á–∏ (30)
        if task_type in model.get('best_for', []):
            score += 30
        elif task_type in ['chat', 'general']:
            score += 15
        
        # 2) –ò—Å—Ç–æ—Ä–∏—è (25)
        perf = self._get_model_performance(model['name'], task_type)
        score += perf['success_rate'] * 25
        
        # 3) –ë—é–¥–∂–µ—Ç (25)
        budget_match = {
            'free': ['ollama', 'gemini-2.0-flash'],
            'cheap': ['deepseek/deepseek-chat', 'gemini-2.0-flash', 'ollama'],
            'medium': ['gpt-4-turbo', 'deepseek/deepseek-chat'],
            'expensive': ['gpt-4o', 'claude-sonnet-4-20250514', 'gpt-4-turbo']
        }
        if model['name'] in budget_match.get(budget, []):
            score += 25
        elif budget == 'expensive':
            score += 15
        
        # 4) –°–ª–æ–∂–Ω–æ—Å—Ç—å (20)
        model_tier = model.get('tier', 'medium')
        if complexity >= 8 and model_tier == 'premium':
            score += 20
        elif complexity >= 5 and model_tier in ['premium', 'advanced']:
            score += 20
        elif complexity < 5 and model_tier in ['basic', 'standard']:
            score += 20
        else:
            score += 10
        
        # –ë–æ–Ω—É—Å –∑–∞ –æ–ø—ã—Ç (–¥–æ 10)
        if perf['total_uses'] > 10:
            score += min(10, perf['total_uses'] / 10)
        
        logger.debug(f"Model {model['name']}: score={score:.1f}, perf={perf}")
        return score
    
    def route(
        self,
        prompt: str,
        task_type: str = 'chat',
        complexity: int = 5,
        budget: str = 'cheap',
        max_retries: int = 3,
        session_id: str = None,
        use_cache: bool = True
    ) -> Dict[str, Any]:
        """
        –£–º–Ω—ã–π —Ä–æ—É—Ç–∏–Ω–≥ —Å –∫—ç—à–µ–º, fallback –∏ –∫–æ–Ω—Ç–µ–∫—Å—Ç–æ–º
        """
        try:
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∫—ç—à (—Ç–æ–ª—å–∫–æ –¥–ª—è –ø—Ä–æ—Å—Ç—ã—Ö –∑–∞–ø—Ä–æ—Å–æ–≤ –±–µ–∑ —Å–µ—Å—Å–∏–∏)
            if use_cache and not session_id:
                cached = self.db.get_cached_response(prompt, task_type)
                if cached:
                    return {
                        'response': cached['response'],
                        'model': cached['model'],
                        'tokens': len(prompt.split()) + len(cached['response'].split()),
                        'cost': 0.0,
                        'cached': True,
                        'cache_age_hours': self._get_cache_age(cached['created_at']),
                        'use_count': cached['use_count']
                    }
            
            # –ö–æ–Ω—Ç–µ–∫—Å—Ç —Å–µ—Å—Å–∏–∏ (–µ—Å–ª–∏ —É–∫–∞–∑–∞–Ω session_id)
            context_messages: List[Dict[str, Any]] = []
            if session_id:
                context_messages = self.db.get_session_context(session_id, max_messages=10)
                logger.info(f"üìö Loaded {len(context_messages)} context messages from session {session_id}")
            
            logger.info(f"Smart routing: task={task_type}, complexity={complexity}, budget={budget}")
            
            # –†–∞—Å—Å—á–∏—Ç—ã–≤–∞–µ–º score –¥–ª—è –∫–∞–∂–¥–æ–π –º–æ–¥–µ–ª–∏
            model_scores: List[Dict[str, Any]] = []
            for model in self.models:
                score = self._calculate_model_score(model, task_type, complexity, budget)
                model_scores.append({'model': model, 'score': score})
            model_scores.sort(key=lambda x: x['score'], reverse=True)
            
            # –ü—Ä–æ–±—É–µ–º –º–æ–¥–µ–ª–∏ –ø–æ –æ—á–µ—Ä–µ–¥–∏ —Å fallback
            attempts: List[Dict[str, Any]] = []
            last_error: Optional[str] = None
            
            for attempt_num in range(min(max_retries, len(model_scores))):
                selected = model_scores[attempt_num]
                model = selected['model']
                try:
                    logger.info(f"üîÑ Attempt {attempt_num + 1}/{max_retries}: {model['name']} (score: {selected['score']:.1f})")
                    # –§–æ—Ä–º–∏—Ä—É–µ–º –ø–æ–ª–Ω—ã–π –ø—Ä–æ–º–ø—Ç —Å –∫–æ–Ω—Ç–µ–∫—Å—Ç–æ–º
                    full_prompt = self._build_prompt_with_context(context_messages, prompt) if context_messages else prompt
                    
                    # –ü—Ä–æ–≤–µ—Ä—è–µ–º rate limit
                    if not self._check_rate_limit(model['name'], wait=True):
                        raise Exception(f"Rate limit exceeded for {model['name']}")

                    # –û—Ç–ø—Ä–∞–≤–ª—è–µ–º –∑–∞–ø—Ä–æ—Å –∫ –º–æ–¥–µ–ª–∏ —á–µ—Ä–µ–∑ —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–π –∏—Å–ø–æ–ª–Ω–∏—Ç–µ–ª—å
                    result = self._execute(model['name'], full_prompt)
                    
                    # –ï—Å–ª–∏ –≤—ã–∑–æ–≤ –≤–µ—Ä–Ω—É–ª—Å—è —Å –æ—à–∏–±–∫–æ–π ‚Äî —Å—á–∏—Ç–∞–µ–º –∫–∞–∫ –ø—Ä–æ–≤–∞–ª –∏ –ø—Ä–æ–±—É–µ–º —Å–ª–µ–¥—É—é—â—É—é –º–æ–¥–µ–ª—å
                    if result.get('error'):
                        raise RuntimeError(result.get('response', 'Unknown model error'))
                    
                    # –£—Å–ø–µ—Ö: –æ–±–Ω–æ–≤–ª—è–µ–º –∞–≥—Ä–µ–≥–∏—Ä–æ–≤–∞–Ω–Ω—É—é —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É
                    self._update_stats(result)

                    # –°–æ—Ö—Ä–∞–Ω—è–µ–º –ø–µ—Ä–µ–ø–∏—Å–∫—É –≤ —Å–µ—Å—Å–∏—é (–µ—Å–ª–∏ —É–∫–∞–∑–∞–Ω–∞)
                    if session_id:
                        try:
                            self.db.add_message_to_session(
                                session_id=session_id,
                                role='user',
                                content=prompt,
                                tokens=len(prompt.split())
                            )
                            self.db.add_message_to_session(
                                session_id=session_id,
                                role='assistant',
                                content=result.get('response', ''),
                                model=model['name'],
                                tokens=result.get('tokens', 0)
                            )
                        except Exception as persist_err:
                            logger.warning(f"Failed to persist session messages: {persist_err}")

                    # –ö—ç—à–∏—Ä—É–µ–º –æ—Ç–≤–µ—Ç (–µ—Å–ª–∏ –Ω–µ—Ç —Å–µ—Å—Å–∏–∏)
                    if use_cache and not session_id:
                        try:
                            self.db.cache_response(
                                prompt=prompt,
                                response=result.get('response', ''),
                                model=model['name'],
                                task_type=task_type,
                                ttl_hours=self._get_cache_ttl(task_type)
                            )
                        except Exception as cache_err:
                            logger.warning(f"Cache store failed: {cache_err}")

                    return {
                        **result,
                        'score': selected['score'],
                        'alternatives': [m['model']['name'] for m in model_scores[1:4]],
                        'attempts': attempt_num + 1,
                        'fallback_used': attempt_num > 0,
                        'failed_models': [a['model'] for a in attempts] if attempts else [],
                        'context_used': bool(context_messages),
                        'context_length': len(context_messages)
                    }
                except Exception as e:
                    logger.warning(f"‚ö†Ô∏è {model['name']} failed: {e}")
                    last_error = str(e)
                    attempts.append({'model': model['name'], 'error': str(e), 'score': selected['score']})
                    continue
            
            # –í—Å–µ –º–æ–¥–µ–ª–∏ –ø—Ä–æ–≤–∞–ª–∏–ª–∏—Å—å
            logger.error(f"‚ùå All {len(attempts)} models failed")
            return {
                'response': f"Error: All models failed. Last error: {last_error}",
                'model': 'none',
                'tokens': 0,
                'cost': 0.0,
                'error': True,
                'failed_models': [a['model'] for a in attempts],
                'attempts': len(attempts)
            }
        except Exception as e:
            logger.error(f"‚ùå Routing error: {e}")
            return {
                'response': f"Error: {str(e)}",
                'model': 'error',
                'tokens': 0,
                'cost': 0.0,
                'error': True
            }

    def _build_prompt_with_context(self, context_messages: List[Dict], current_prompt: str) -> str:
        """
        –ü–æ—Å—Ç—Ä–æ–∏—Ç—å –ø—Ä–æ–º–ø—Ç —Å —É—á–µ—Ç–æ–º –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞
        """
        context_text = "Previous conversation:\n\n"
        for msg in context_messages[-6:]:  # –ø–æ—Å–ª–µ–¥–Ω–∏–µ 6 —Å–æ–æ–±—â–µ–Ω–∏–π
            role = "User" if msg.get('role') == 'user' else "Assistant"
            content = msg.get('content', '')
            context_text += f"{role}: {content}\n\n"
        full_prompt = f"{context_text}User: {current_prompt}\n\nAssistant:"
        return full_prompt

    def _get_cache_age(self, created_at: str) -> float:
        """–í—ã—á–∏—Å–ª–∏—Ç—å –≤–æ–∑—Ä–∞—Å—Ç –∫—ç—à–∞ –≤ —á–∞—Å–∞—Ö"""
        from datetime import datetime
        try:
            created = datetime.fromisoformat(created_at)
            age = (datetime.now() - created).total_seconds() / 3600
            return round(age, 1)
        except Exception:
            return 0.0

    def _get_cache_ttl(self, task_type: str) -> int:
        """
        –û–ø—Ä–µ–¥–µ–ª–∏—Ç—å TTL –¥–ª—è –∫—ç—à–∞ –≤ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –æ—Ç —Ç–∏–ø–∞ –∑–∞–¥–∞—á–∏
        """
        ttl_map = {
            'chat': 1,
            'general': 1,
            'code': 24,
            'review': 24,
            'architecture': 48,
            'test': 24,
            'devops': 12,
            'research': 168
        }
        return ttl_map.get(task_type, 24)
    
    def _select_model(
        self,
        task_type: str,
        complexity: int,
        budget: str
    ) -> str:
        """
        –ú–∞—Ç—Ä–∏—Ü–∞ –≤—ã–±–æ—Ä–∞ –º–æ–¥–µ–ª–∏ –ø–æ –∑–∞–¥–∞—á–µ
        
        –ü—Ä–∏–æ—Ä–∏—Ç–µ—Ç—ã:
        - architecture: –≤—Å–µ–≥–¥–∞ Claude (—Å–∞–º—ã–π —É–º–Ω—ã–π)
        - code: GPT-4 –∏–ª–∏ DeepSeek –ø–æ —Å–ª–æ–∂–Ω–æ—Å—Ç–∏
        - review: Gemini (–±—ã—Å—Ç—Ä–æ –∏ –±–µ—Å–ø–ª–∞—Ç–Ω–æ) –∏–ª–∏ DeepSeek
        - test: GPT-4 –∏–ª–∏ Gemini
        - devops: DeepSeek (—Å–ø–µ—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–π)
        - research: Claude (–ª—É—á—à–µ–µ –º—ã—à–ª–µ–Ω–∏–µ)
        """
        
        # –ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞ - –≤—Å–µ–≥–¥–∞ –ª—É—á—à–∞—è –º–æ–¥–µ–ª—å
        if task_type == 'architecture':
            if self.claude:
                return 'claude-sonnet-4-20250514'
            elif self.openai:
                return 'gpt-4-turbo'
            else:
                return 'ollama'
        
        # –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –∫–æ–¥–∞ - –ø–æ —Å–ª–æ–∂–Ω–æ—Å—Ç–∏
        if task_type == 'code':
            if budget == 'free':
                if self.gemini:
                    return 'gemini-2.0-flash'
                else:
                    return 'ollama'
            elif complexity <= 6:
                if self.openrouter:
                    return 'deepseek/deepseek-chat'
                elif self.openai:
                    return 'gpt-4-turbo'
                else:
                    return 'ollama'
            else:
                if self.openai:
                    return 'gpt-4-turbo'
                elif self.claude:
                    return 'claude-sonnet-4-20250514'
                else:
                    return 'ollama'
        
        # –†–µ–≤—å—é –∫–æ–¥–∞ - –±—ã—Å—Ç—Ä—ã–µ –º–æ–¥–µ–ª–∏
        if task_type == 'review':
            if budget == 'free':
                if self.gemini:
                    return 'gemini-2.0-flash'
                else:
                    return 'ollama'
            else:
                if self.openrouter:
                    return 'deepseek/deepseek-chat'
                elif self.gemini:
                    return 'gemini-2.0-flash'
                else:
                    return 'ollama'
        
        # –¢–µ—Å—Ç—ã - —Å—Ä–µ–¥–Ω–∏–µ –º–æ–¥–µ–ª–∏
        if task_type == 'test':
            if budget != 'free' and self.openai:
                return 'gpt-4-turbo'
            elif self.gemini:
                return 'gemini-2.0-flash'
            else:
                return 'ollama'
        
        # DevOps - —Å–ø–µ—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ
        if task_type == 'devops':
            if self.openrouter:
                return 'deepseek/deepseek-chat'
            elif self.openai:
                return 'gpt-4-turbo'
            else:
                return 'ollama'
        
        # –ò—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏—è - –ª—É—á—à–µ–µ –º—ã—à–ª–µ–Ω–∏–µ
        if task_type == 'research':
            if self.claude:
                return 'claude-sonnet-4-20250514'
            elif self.openai:
                return 'gpt-4-turbo'
            else:
                return 'ollama'
        
        # –î–µ—Ñ–æ–ª—Ç
        return 'ollama'
    
    def _execute(self, model: str, prompt: str) -> Dict:
        """–í—ã–ø–æ–ª–Ω–µ–Ω–∏–µ –∑–∞–ø—Ä–æ—Å–∞ –∫ –≤—ã–±—Ä–∞–Ω–Ω–æ–π –º–æ–¥–µ–ª–∏"""
        
        if 'claude' in model and self.claude:
            return self._call_claude(model, prompt)
        elif 'gpt' in model and self.openai:
            return self._call_openai(model, prompt)
        elif 'deepseek' in model and self.openrouter:
            return self._call_openrouter(model, prompt)
        elif 'gemini' in model and self.gemini:
            return self._call_gemini(prompt)
        elif model == 'ollama':
            return self._call_ollama(prompt)
        else:
            raise ValueError(f"Model {model} not available or not configured")
    
    def _call_claude(self, model: str, prompt: str) -> Dict:
        """–í—ã–∑–æ–≤ Claude API"""
        try:
            response = self.claude.messages.create(
                model=model,
                max_tokens=4000,
                messages=[{"role": "user", "content": prompt}]
            )
            
            return {
                'response': response.content[0].text,
                'model': model,
                'tokens': response.usage.input_tokens + response.usage.output_tokens,
                'cost': self._calculate_cost(model, {
                    'input_tokens': response.usage.input_tokens,
                    'output_tokens': response.usage.output_tokens
                }),
                'error': False
            }
        except Exception as e:
            return {
                'response': f"Claude Error: {str(e)}",
                'model': model,
                'tokens': 0,
                'cost': 0.0,
                'error': True
            }
    
    def _call_openai(self, model: str, prompt: str) -> Dict:
        """–í—ã–∑–æ–≤ OpenAI API"""
        try:
            response = self.openai.chat.completions.create(
                model=model,
                messages=[{"role": "user", "content": prompt}]
            )
            
            return {
                'response': response.choices[0].message.content,
                'model': model,
                'tokens': response.usage.total_tokens,
                'cost': self._calculate_cost(model, {
                    'input_tokens': response.usage.prompt_tokens,
                    'output_tokens': response.usage.completion_tokens
                }),
                'error': False
            }
        except Exception as e:
            return {
                'response': f"OpenAI Error: {str(e)}",
                'model': model,
                'tokens': 0,
                'cost': 0.0,
                'error': True
            }
    
    def _call_openrouter(self, model: str, prompt: str) -> Dict:
        """–í—ã–∑–æ–≤ —á–µ—Ä–µ–∑ OpenRouter"""
        try:
            response = self.openrouter.chat.completions.create(
                model=model,
                messages=[{"role": "user", "content": prompt}]
            )
            
            return {
                'response': response.choices[0].message.content,
                'model': model,
                'tokens': response.usage.total_tokens if response.usage else 0,
                'cost': self._calculate_cost(model, {
                    'input_tokens': response.usage.prompt_tokens if response.usage else 0,
                    'output_tokens': response.usage.completion_tokens if response.usage else 0
                }),
                'error': False
            }
        except Exception as e:
            return {
                'response': f"OpenRouter Error: {str(e)}",
                'model': model,
                'tokens': 0,
                'cost': 0.0,
                'error': True
            }
    
    def _call_gemini(self, prompt: str) -> Dict:
        """–í—ã–∑–æ–≤ Google Gemini"""
        try:
            response = self.gemini.generate_content(prompt)
            
            return {
                'response': response.text,
                'model': 'gemini-2.0-flash',
                'tokens': int(len(prompt.split()) * 1.3),  # –ø—Ä–∏–±–ª–∏–∑–∏—Ç–µ–ª—å–Ω–æ
                'cost': 0.0,  # –ë–µ—Å–ø–ª–∞—Ç–Ω–æ –¥–æ –ª–∏–º–∏—Ç–∞
                'error': False
            }
        except Exception as e:
            return {
                'response': f"Gemini Error: {str(e)}",
                'model': 'gemini-2.0-flash',
                'tokens': 0,
                'cost': 0.0,
                'error': True
            }
    
    def _call_ollama(self, prompt: str) -> Dict:
        """–õ–æ–∫–∞–ª—å–Ω—ã–π –∑–∞–ø—Ä–æ—Å —á–µ—Ä–µ–∑ Ollama"""
        try:
            import subprocess
            import json
            
            # –í—ã–∑–æ–≤ Ollama CLI
            result = subprocess.run(
                ['ollama', 'run', 'qwen2.5-coder:14b', prompt],
                capture_output=True,
                text=True,
                timeout=60
            )
            
            if result.returncode == 0:
                return {
                    'response': result.stdout.strip(),
                    'model': 'ollama-qwen2.5-coder:14b',
                    'tokens': int(len(prompt.split()) * 1.5),
                    'cost': 0.0,  # –õ–æ–∫–∞–ª—å–Ω–æ –±–µ—Å–ø–ª–∞—Ç–Ω–æ
                    'error': False
                }
            else:
                return {
                    'response': f"Ollama Error: {result.stderr}",
                    'model': 'ollama',
                    'tokens': 0,
                    'cost': 0.0,
                    'error': True
                }
        except Exception as e:
            return {
                'response': f"Ollama Error: {str(e)}. Make sure Ollama is running.",
                'model': 'ollama',
                'tokens': 0,
                'cost': 0.0,
                'error': True
            }
    
    def _calculate_cost(self, model: str, usage: Dict) -> float:
        """–†–∞—Å—á–µ—Ç —Å—Ç–æ–∏–º–æ—Å—Ç–∏ –∑–∞–ø—Ä–æ—Å–∞ –≤ USD"""
        # –¶–µ–Ω—ã –∑–∞ 1M —Ç–æ–∫–µ–Ω–æ–≤ (input, output)
        costs = {
            'claude-sonnet-4-20250514': (3.0, 15.0),
            'gpt-4-turbo': (10.0, 30.0),
            'gpt-4o': (5.0, 15.0),
            'deepseek/deepseek-chat': (0.27, 1.1),
            'gemini-2.0-flash': (0.0, 0.0),
            'ollama': (0.0, 0.0)
        }
        
        if model not in costs:
            return 0.0
        
        input_cost, output_cost = costs[model]
        
        total = (
            usage.get('input_tokens', 0) / 1_000_000 * input_cost +
            usage.get('output_tokens', 0) / 1_000_000 * output_cost
        )
        
        return round(total, 6)
    
    def _update_stats(self, result: Dict):
        """–û–±–Ω–æ–≤–ª–µ–Ω–∏–µ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è"""
        if result.get('error'):
            return
        
        self.stats['calls'] += 1
        self.stats['tokens'] += result.get('tokens', 0)
        self.stats['cost'] += result.get('cost', 0.0)
        
        model = result.get('model', 'unknown')
        if model not in self.stats['by_model']:
            self.stats['by_model'][model] = {
                'calls': 0,
                'tokens': 0,
                'cost': 0.0
            }
        
        self.stats['by_model'][model]['calls'] += 1
        self.stats['by_model'][model]['tokens'] += result.get('tokens', 0)
        self.stats['by_model'][model]['cost'] += result.get('cost', 0.0)
    
    def get_stats(self) -> Dict:
        """–ü–æ–ª—É—á–∏—Ç—å —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è"""
        return {
            **self.stats,
            'avg_cost_per_call': self.stats['cost'] / max(self.stats['calls'], 1),
            'available_models': self._get_available_models()
        }
    
    def _get_available_models(self) -> Dict[str, bool]:
        """–ü—Ä–æ–≤–µ—Ä–∫–∞ –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç–∏ –º–æ–¥–µ–ª–µ–π"""
        return {
            'claude': self.claude is not None,
            'openai': self.openai is not None,
            'openrouter': self.openrouter is not None,
            'gemini': self.gemini is not None,
            'ollama': True  # –í—Å–µ–≥–¥–∞ –¥–æ—Å—Ç—É–ø–µ–Ω –ª–æ–∫–∞–ª—å–Ω–æ
        }

# –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –ø—Ä–∏ –ø—Ä—è–º–æ–º –∑–∞–ø—É—Å–∫–µ
if __name__ == "__main__":
    print("ü§ñ AI Router Test\n")
    
    router = AIRouter()
    
    # –ü—Ä–æ–≤–µ—Ä–∫–∞ –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç–∏ –º–æ–¥–µ–ª–µ–π
    print("üìä Available Models:")
    for model, available in router._get_available_models().items():
        status = "‚úÖ" if available else "‚ùå"
        print(f"  {status} {model}")
    
    print("\n" + "="*50 + "\n")
    
    # –¢–µ—Å—Ç –ø—Ä–æ—Å—Ç–æ–≥–æ –∑–∞–ø—Ä–æ—Å–∞
    print("üß™ Testing simple code generation...")
    result = router.route(
        "Write a Python function to calculate fibonacci numbers",
        task_type='code',
        complexity=3,
        budget='free'
    )
    
    print(f"Model used: {result['model']}")
    print(f"Cost: ${result['cost']:.6f}")
    print(f"Tokens: {result['tokens']}")
    print(f"\nResponse preview:\n{result['response'][:200]}...")
    
    print("\n" + "="*50 + "\n")
    
    # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
    stats = router.get_stats()
    print("üìà Statistics:")
    print(f"  Total calls: {stats['calls']}")
    print(f"  Total tokens: {stats['tokens']}")
    print(f"  Total cost: ${stats['cost']:.6f}")
    print(f"  Avg cost/call: ${stats['avg_cost_per_call']:.6f}")